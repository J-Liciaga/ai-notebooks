{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liquid Neural Network\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Liquid Neural Networks (LNNs) are a type of neural network architecture that is designed to be much more robust and adaptable than traditional neural networks. LNNs are inspired by the fluid dynamics of liquids, which allows them to process data in a way that is similar to how liquids flow through a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LiquidNeuronNetwork(nn.Module):\n",
    "    def __init_(self, input_size, output_size):\n",
    "        super(LiquidNeuronNetwork, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random dummy data\n",
    "X = torch.arand(100, 1)\n",
    "y = 2 * X + 1 + torch.arand(100, 1) * 0.1\n",
    "\n",
    "# create a dataloader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# initialize the model, loss function, and optimizer\n",
    "model = LiquidNeuronNetwork(1, 1)\n",
    "lossfn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        # forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = lossfn(outputs, batch_y)\n",
    "\n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # optmize\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"Loss: {loss.item():.4f}\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "with torch.no_grad():\n",
    "    X_test = torch.tensor([[0.0], [1.0], [2.0], [3.0]])\n",
    "    y_pred = model(X_test)\n",
    "    print(f\"Predictions: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.scatter(X, y, label='Data')\n",
    "plt.plot(X_test, y_pred, 'r', label='Prediction')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended testing\n",
    "def test_model(model, X_test, y_test=None):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "\n",
    "        print(\"Input (X) | Predicted (y)\")\n",
    "        print(\"-\" * 25)\n",
    "\n",
    "        for x, y in zip(X_test, y_pred):\n",
    "            print(f\"{x.item():8.3f} | {y.item():8.3f}\")\n",
    "        \n",
    "        if y_test is not None:\n",
    "            mse = nn.MSELoss()(y_pred, y_test)\n",
    "            print(f\"\\n MSE: {mse.item():.4f}\")\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate more test data\n",
    "X_extended_test = torch.tensor([[-2.0], [-1.0], [0.0], [1.0], [2.0], [3.0], [4.0]])\n",
    "y_extended_test = 2 * X_extended_test + 1 + torch.randn(X_extended_test.shape) * 0.1\n",
    "\n",
    "# Test the model\n",
    "print(\"Model Test Results:\")\n",
    "y_pred = test_model(model, X_test, y_extended_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, label='Training Data', alpha=0.5)\n",
    "plt.scatter(X_extended_test, y_extended_test, label='Test Data', color='green', alpha=0.5)\n",
    "plt.plot(X_test, y_pred, 'r', label='Prediction')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('LNN Predictions vs Actual Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check learned parameters\n",
    "print(\"\\n Learned Model Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data.numpy().flatten()}\")\n",
    "\n",
    "# Compare with true function\n",
    "true_slope, true_intercept = 2, 1\n",
    "learned_slope = model.linear.weight.item()\n",
    "learned_intercept = model.linear.bias.item()\n",
    "\n",
    "print(f\"\\n True function: y = {true_slope}x + {true_intercept}\")\n",
    "print(f\"Learned function: y = {learned_slope:.4f}x + {learned_intercept:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
