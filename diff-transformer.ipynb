{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794cb26d-4dee-4ff7-b1ec-ffbc6a05de3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (4.12.1)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0579793d-145d-4fbf-98d4-44baae3d800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2103bc-d880-4af0-b81c-71c12392e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf3c6f0-adcb-459f-bf11-bcee4c6d091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(DiffMultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def diff_attention(self, Q, K, V):\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        A1 = F.softmax(scores, dim=-1)\n",
    "        A2 = F.softmax(scores, dim=-1)\n",
    "        diff = A1 - A2\n",
    "        \n",
    "        return torch.matmul(diff, V)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        output = self.diff_attention(q, k, v)\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.dense(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ac657c-5d37-493e-8df8-21b147b9b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.linear_a = nn.Linear(d_model, d_ff)\n",
    "        self.linear_b = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_b(nn.ReLU()(self.linear_a(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02e4347-d81d-49b7-9231-b1033b9d1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.diff_multi_head_attention = DiffMultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        self.layernorm_1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm_2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attention_output = self.diff_multi_head_attention(x, x, x, mask)\n",
    "        out1 = self.layernorm_1(x + self.dropout(attention_output))\n",
    "\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        out2 = self.layernorm_2(out1 + self.dropout(ff_output))\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee529a9-8879-44dc-b7f5-ad74d69bc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.diff_multi_head_attention_1 = DiffMultiHeadAttention(d_model, num_heads)\n",
    "        self.diff_multi_head_attention_2 = DiffMultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "\n",
    "        self.layernorm_1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm_2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm_3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attention_output_1 = self.diff_multi_head_attention_1(x, x, x, look_ahead_mask)\n",
    "        out1 = self.layernorm_1(x + self.dropout(attention_output_1))\n",
    "\n",
    "        attention_output_2 = self.diff_multi_head_attention_2(out1, enc_output, enc_output, padding_mask)\n",
    "        out2 = self.layernorm_2(out1 + self.dropout(attention_output_2))\n",
    "\n",
    "        ff_output = self.feed_forward(out2)\n",
    "        out3 = self.layernorm_3(out2 + self.dropout(ff_output))\n",
    "\n",
    "        return out3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e743cb24-e906-4283-89a2-1c6dc373d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size, target_vocab_size, max_seq_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList(EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers))\n",
    "        self.decoder_layers = nn.ModuleList(DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers))\n",
    "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.encoder_embedding(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        return src\n",
    "    \n",
    "    def decode(self, target, memory, target_mask, src_mask):\n",
    "        tgt = self.decoder_embedding(target)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, memory, target_mask, src_mask)\n",
    "\n",
    "        return tgt\n",
    "    \n",
    "    def forward(self, src, target, src_mask, target_mask):\n",
    "        enc_output = self.encode(src, src_mask)\n",
    "        dec_output = self.decode(target, enc_output, target_mask, src_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8176d06c-19e8-4cee-ba00-17d66c2be8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_len = 100  # Make sure this matches or exceeds your maximum sequence length\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, d_ff, src_vocab_size, tgt_vocab_size, max_seq_len, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e15dde0a-9999-452c-8dd2-16544ad651b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50, 5000])\n",
      "Epoch 0, Loss: 8.671402931213379\n",
      "Epoch 10, Loss: 7.059451103210449\n",
      "Epoch 20, Loss: 5.545629978179932\n",
      "Epoch 30, Loss: 4.181428909301758\n",
      "Epoch 40, Loss: 3.055572748184204\n",
      "Epoch 50, Loss: 2.170642614364624\n",
      "Epoch 60, Loss: 1.4853633642196655\n",
      "Epoch 70, Loss: 0.9765584468841553\n",
      "Epoch 80, Loss: 0.6180844306945801\n",
      "Epoch 90, Loss: 0.3937240242958069\n",
      "Epoch 100, Loss: 0.2623292803764343\n",
      "Epoch 110, Loss: 0.18651872873306274\n",
      "Epoch 120, Loss: 0.13920468091964722\n",
      "Epoch 130, Loss: 0.10877159237861633\n",
      "Epoch 140, Loss: 0.08754243701696396\n",
      "Epoch 150, Loss: 0.0723409429192543\n",
      "Epoch 160, Loss: 0.060705047100782394\n",
      "Epoch 170, Loss: 0.05175051838159561\n",
      "Epoch 180, Loss: 0.04458371177315712\n",
      "Epoch 190, Loss: 0.03848520666360855\n",
      "Epoch 200, Loss: 0.03357745334506035\n",
      "Epoch 210, Loss: 0.02946525812149048\n",
      "Epoch 220, Loss: 0.02588602527976036\n",
      "Epoch 230, Loss: 0.022866351529955864\n",
      "Epoch 240, Loss: 0.020365538075566292\n",
      "Epoch 250, Loss: 0.01803489215672016\n",
      "Epoch 260, Loss: 0.016027770936489105\n",
      "Epoch 270, Loss: 0.014311266131699085\n",
      "Epoch 280, Loss: 0.012788387946784496\n",
      "Epoch 290, Loss: 0.011377807706594467\n",
      "Epoch 300, Loss: 0.010193338617682457\n",
      "Epoch 310, Loss: 0.009174850769340992\n",
      "Epoch 320, Loss: 0.00827379897236824\n",
      "Epoch 330, Loss: 0.007400125730782747\n",
      "Epoch 340, Loss: 0.006693780887871981\n",
      "Epoch 350, Loss: 0.00603077095001936\n",
      "Epoch 360, Loss: 0.005430945660918951\n",
      "Epoch 370, Loss: 0.004893574398010969\n",
      "Epoch 380, Loss: 0.004422138445079327\n",
      "Epoch 390, Loss: 0.003977418411523104\n",
      "Epoch 400, Loss: 0.0036088444758206606\n",
      "Epoch 410, Loss: 0.0032476491760462523\n",
      "Epoch 420, Loss: 0.002953453455120325\n",
      "Epoch 430, Loss: 0.0026633916422724724\n",
      "Epoch 440, Loss: 0.0024105936754494905\n",
      "Epoch 450, Loss: 0.0021884364541620016\n",
      "Epoch 460, Loss: 0.001977080712094903\n",
      "Epoch 470, Loss: 0.0017865492263808846\n",
      "Epoch 480, Loss: 0.0016269076149910688\n",
      "Epoch 490, Loss: 0.0014759679324924946\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# sample input\n",
    "batch_size = 64\n",
    "seq_length = 50\n",
    "src = torch.randint(1, src_vocab_size, (batch_size, seq_length))\n",
    "tgt = torch.randint(1, tgt_vocab_size, (batch_size, seq_length)) \n",
    "\n",
    "# create masks \n",
    "src_mask = torch.ones(batch_size, 1, 1, seq_length)\n",
    "tgt_mask = torch.tril(torch.ones(batch_size, 1, seq_length, seq_length))\n",
    "\n",
    "# forward pass\n",
    "output = transformer(src, tgt, src_mask, tgt_mask)\n",
    "print(output.shape)  # Expected: torch.Size([64, 50, 5000])\n",
    "\n",
    "# training loop \n",
    "epochs = 500\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0) \n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src, tgt, src_mask, tgt_mask)\n",
    "    loss = loss_fn(output.view(-1, tgt_vocab_size), tgt.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
